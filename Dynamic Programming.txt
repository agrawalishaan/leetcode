Try DP from both directions.

In 2d dp, we sometimes don't need to allocate an entire matrix of memory. For instance in 62: Unique Paths, we start tabulating from the bottom right, moving left and up. Each dp cell is the sum of the one below it, and to the right. So we can just maintain a 1d array and overwrite within the same array as we tabulate.

Instead of using memoization for a 2d cache, like JSON.stringify([row, col]), it's usually faster to allocate a matrix instead, as stringification and hashtable dereferencing are slow. We can initialize values in the matrix to be a certain type to indicate they haven't been solved.

A common strategy in dp is to start iterating backwards, solving prior values based on future ones. For each prior value, we iterate through n future values. This is an n^2 solution but is used in many problems such as 1027: Longest Arithmetic Subsequence. We can also usually solve it from the beginning, looking at n prior elements (either left to right or right to left), but usually I like to solve from the back first. We can also do this on some stock problems, for instance in buying and selling a stock with at most 2 transactions, our solution at a given ending index would be to buy a stock at that price, iterate through all future values, sell at a future value, and take the next dp which stores the results for buying and selling with only 1 transaction allowed. Or in buying and selling with transaction fee, we could take a value, assume we buy there, iterate through all future values, assume we sell at that future value, and add the dp on the right sub problem. A good example of this in code is Buy and Sell Stock with Cooldown

If we are solving something like a 2d dp problem, we can allocate a larger array and store 0s or 1s or whatever we need on the edges, to prevent out of bounds cases. But sometimes it is easier to just check if we are out of bounds, because if we allocate a larger matrix the index conversion between the input grid and our larger allocation can be a bit confusing.

State machines rely on updating values based on prior values. See the buy and sell stock problems (1-4, with transaction fee, with cooldown, for examples).

DP is pretty flexible. For instance in 2745: Construct the Longest New String (we have AA, BB, and AB, need to make longest string with AAA or BBB), we can basically take a starting pair of letters, consider the ending letters of it, and use that to determine adjacent states, along with how many x, y, and z we have remaining.

The amount of states in a DP memoization does not determine the time complexity, but is related. For instance in 1140: Stone Game II (we can take from the first 2*m piles, maximize stones), there are n^2 states to memoize, defined by starting index and m. Basically there are n subarrays with n possible range. And to solve a given dp cell, we need to iterate through up to n possible range, so n^3 time.

Consider a question 'delete string'. we are given some string of chars, say abcb, and we can delete any contiguous block of letters. Find the minimum number of deletions to delete the string (this problem is very similar to 664: Strange Printer). One naive way of doing this is:

Try deleting any possible block, of which there are at most n choices, then solve the sub problem for an array of size n-1. For instance, try deleting a and solve bcb, try deleting the first b and solving acb, etc etc. This is n factorial. We could instead try memoizing solutions based on what letters are left, so now abcb when we delete a->c, we get the same memoized result as c->a. This now means the order does NOT matter, rather just the total amount of operations we have taken to reach a state. Since there are 2^n possible states (an element is either present or missing), the main time complexity factor becomes 2^n. Finally, we can see if we can instead use a n^2 problem representation. If we delete a letter, we can delete it at various points. Either immediately, separately from the rest, or if there is a matching letter first, we can delete everything in between first, then that group. Hence n^2 sub problems, each taking n time to solve.