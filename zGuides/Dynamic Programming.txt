Try DP from both directions.

In 2d dp, we sometimes don't need to allocate an entire matrix of memory. For instance in 62: Unique Paths, we start tabulating from the bottom right, moving left and up. Each dp cell is the sum of the one below it, and to the right. So we can just maintain a 1d array and overwrite within the same array as we tabulate.

Instead of using memoization for a 2d cache, like JSON.stringify([row, col]), it's usually faster to allocate a matrix instead, as stringification and hashtable dereferencing are slow. We can initialize values in the matrix to be a certain type to indicate they haven't been solved.

A common strategy in dp is to start iterating backwards, solving prior values based on future ones. For each prior value, we iterate through n future values. This is an n^2 solution but is used in many problems such as 1027: Longest Arithmetic Subsequence. We can also usually solve it from the beginning, looking at n prior elements (either left to right or right to left), but usually I like to solve from the back first. We can also do this on some stock problems, for instance in buying and selling a stock with at most 2 transactions, our solution at a given ending index would be to buy a stock at that price, iterate through all future values, sell at a future value, and take the next dp which stores the results for buying and selling with only 1 transaction allowed. Or in buying and selling with transaction fee, we could take a value, assume we buy there, iterate through all future values, assume we sell at that future value, and add the dp on the right sub problem. A good example of this in code is Buy and Sell Stock with Cooldown

If we are solving something like a 2d dp problem, we can allocate a larger array and store 0s or 1s or whatever we need on the edges, to prevent out of bounds cases. But sometimes it is easier to just check if we are out of bounds, because if we allocate a larger matrix the index conversion between the input grid and our larger allocation can be a bit confusing.

State machines rely on updating values based on prior values. See the buy and sell stock problems (1-4, with transaction fee, with cooldown, for examples).

DP is pretty flexible. For instance in 2745: Construct the Longest New String (we have AA, BB, and AB, need to make longest string with AAA or BBB), we can basically take a starting pair of letters, consider the ending letters of it, and use that to determine adjacent states, along with how many x, y, and z we have remaining.

The amount of states in a DP memoization does not determine the time complexity, but is related. For instance in 1140: Stone Game II (we can take from the first 2*m piles, maximize stones), there are n^2 states to memoize, defined by starting index and m. Basically there are n subarrays with n possible range. And to solve a given dp cell, we need to iterate through up to n possible range, so n^3 time.

Consider a question 'delete string'. we are given some string of chars, say abcb, and we can delete any contiguous block of letters. Find the minimum number of deletions to delete the string (this problem is very similar to 664: Strange Printer). One naive way of doing this is:

Try deleting any possible block, of which there are at most n choices, then solve the sub problem for an array of size n-1. For instance, try deleting a and solve bcb, try deleting the first b and solving acb, etc etc. This is n factorial. We could instead try memoizing solutions based on what letters are left, so now abcb when we delete a->c, we get the same memoized result as c->a. This now means the order does NOT matter, rather just the total amount of operations we have taken to reach a state. Since there are 2^n possible states (an element is either present or missing), the main time complexity factor becomes 2^n. Finally, we can see if we can instead use a n^2 problem representation. If we delete a letter, we can delete it at various points. Either immediately, separately from the rest, or if there is a matching letter first, we can delete everything in between first, then that group. Hence n^2 sub problems, each taking n time to solve.

some types of recurrence relationships:
1) If we can match the beginning to the end, do so, otherwise take some subproblem of l+1,r and l,r-1, such as longest palindromic subsequence.
2) For each letter, iterate through the right letters, find a match. try those.

The time complexity for a dp problem is the number of states we solve multiplied by the time each state takes. For instance in 1510: Stone Game IV (alice and bob take turns drawing a square root amount of stones). We solve for up to n states, and each state iterates through root n other dps, so n*root n time.

Sometimes when considering adjacent states, we can optimize. In 691: Stickers to Spell Word, where we have to get letters from stickers to spell a target, we can just consider taking stickers that give us the first letter we are missing, as we weould eventually need that letter.

We can serialize arrays or mappings for DP cache lookups like in 691: Stickers to Spell Word (we serialize a mapping of how many letters we have), or 1799: Maximize Score After N Operations (remove pairs and get GCD as score), serialize the actual array of what is left. But we can also do bitmasking for some of these. We can also, instead of serializing, just mutate the same array, call the neighbor then fix the mutation. Copying is usually much easier to implement though.

A common dp pattern is taking the problem, solving something for the first character, and progressing the problem. In 97: Interleaving String (see if s3 can be formed by interleaving s1 and s2 somehow), we try solving for the first letter, and getting the remaining subproblem.

If you want to not allocate a matrix, you can hash multiple parameters like const key=`${var1},${var2}` and store them in a hashmap. But I haven't yet seen a problem where this is good.

In a problem like unbounded knapsack, we can either consider an item (ifTake vs ifSkip), or consider all possible items from our current index. I usually do the first since it is easier to understand. The second would be something like say we are at i=5, meaning we have [i:] elements left to consider. We could try every item, from i to the end, with the new appropriate dp (we can only take items from [i:]).

The memo does NOT need to be the answer to the overarching problem. For instance in the number of palindromic substrings, we can store a memo if [l:r] is a palindrome. And as we solve, we iterate from our starting letter, looking at all future letters that match, seeing if the inside is a palindrome, adding one to our result.

Remember that top down doesn't solve every subproblem, only needed ones, but true dp does. If for instance, we need to solve a problem that requires knowing if a substring is a palindrome (like 1745: palindrome partition IV, split into three palindromes), we can either precompute n^2 palindromes using true dp, or just make a helper function isPalindrome which accesses a cache or recursively calls itself if needed.

In 656: Coin Path, where we need the path for the lowest price to reach the end, we also store the path inside the dp. It is worth noting we can store multiple things in the DP but only answer problems based on one value (the cost), and just use that extra information as per the return statement.

We don't need to necessarily enumerate all ways to find the solution to 'how many ways can we do this'. This is especially obvious if it tells us to return the answer % MOD. In these cases, usually we generate a math based relationship instead of enumerating all the possibilities. And we do something like: "answer tto this subproblem is these two subproblems added or multiplied together"


******** DIGIT DP ********

Digit DP is useful when we need to generate an amount of numbers under a high barrier that fit some property with its digits. Say we have to check all numbers from [1, n] for some property. If n is of length 3, we can generate all numbers with 3 digits. To generate single digit and double digit numbers, we would use leading zeroes.

Digit DP often takes many params such as:
tight - tells us if we are bound on the current digit. Say n is 123. We are initially tight. Therefore, for the first digit we can only choose 0 or 1. If we choose 0, we are smaller than n for sure, and we become unbound on any future digit, no longer tight. If we choose 1, we are still tight, and still bound on the future digit. So generally our newTight is `if tight and digit == upperBoundary`.

nonZeroTaken - We want to know if we have taken a non-zero yet, or we are still leading zeroes. This helps us ensure we don't count the 000 case as a number (though we could just subtract 1 from our result), as well as can be useful for other things. For instance if we haven't taken a nonZero yet, we may have free reign of our digit, but once we have, we may be constrained.

i - this tells us where in the number we are inserting the digit, so we know when we are at the length of n.

If we are to solve for [low, high] instead of [1, n], we can just do [1, high] - [1, low - 1]. We could also use a lower tight bound and pad the low with 0s to make it the same length as high.