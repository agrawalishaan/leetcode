Matrix DFS / BFS:

Some pro tips, we can check in bounds like:

for (const [rowDiff, colDiff] of diffs) {
  const newRow = r + rowDiff;
  const newCol = c + colDiff;
  if (newRow < HEIGHT && newRow >= 0 && newCol < WIDTH && newCol >= 0 && !visited[newRow][newCol] && grid[newRow][newCol] === 1) {
      total += sizeOfIsland(newRow, newCol);
  }
}

If we don't want a huge matrix to store visited/unvisited cells, we could use a set and make keys by WIDTH*row + col. This isn't necessarily more efficient though, it depends how many keys we store. It is also slower to access. It's also harder to convert keys back and forth.

In problem 417: Pacific Atlantic Water Flow, there were a few lessons.
1) Consideer iterating from outside the cells (the edges) instead of inside. This also occured in 130: Surrounding Regions (trap islands surrounded by water)
2) Assuming we can flow from equal height cells, we need a visited set. This prevents us from caching things well though, because in:
5 5 5 5
5 3 3 1
5 5 5 5

If we solve for the second 3, we might recurse to the first 3. And the first 3 cannot flow out, because the second 3 is marked as visited. But the first 3 can actually flow out. So here, we only end up really solving for the root, and anything along it's path to the edge.
The problem itself has a good detailed explanation.

I think that between pacific atlantic water flow, and trapping rainwater II, I've seen that if we need to flow out of a cell, we can only check root cells + cells along their flow path. And finding the bottleneck height seems very hard.

In 695: Max Area of Island, we can either not recurse to invalid adjacent cells at all, or recurse to them, then return 0 if they are invalid. IMO the first is more intuitive, but many people do the second. It is similar to backtracking, just do whatever feels more natural.

In problem 695, we also don't need to set cells to be unvisited later. Since we're just trying to visit everything once, rather than find a path, we only need to set them to true.

In 827, making a large island (we can flip one water tile, find largest island), I tried writing code where we could recurse to invalid neighbors, then terminate via base cases, just for practice. Actually I think it was a bit easier.

In BFS, if we start from the top left of some island and are finding the area, or generally expanding out (for instance finding the shortest path from the top left cell to the bottom right, with walls), our queue size is limited to min(n,m), since in the worst case our queue expands as a diagonal (draw a picture of the bfs for a square matrix to see this). This can be seen in questions: 1091, shortest path in binary matrix, or 200: number of islands. I think most of these types of questions could use less space this way, since queue size is limited.
Even if we start in the middle of the grid, our BFS queue size is limited to the permiter of what we have explored, but our seen set might still be n*m space.

When we do a BFS, say to find the shortest path from one cell to another, the complexity is capped at # of cells that are within range L, where L is the distance between the two cells (this is due to the visited matrix). Whereas in DFS, we don't have certainty that we reached a cell in the shortest possible path. However, in a grid where we can only move right/down, and we want to get to the bottom right cell, DFS/BFS are the same, assuming the cost of a move is always 1. It's true that in some matrices with walls, the DFS and BFS solutions have the same cost, but in general BFS will have a lower cap than DFS (still unsure about the true DFS TC).


Adjaceny List:

Often instead we are given an adjacency list, which maps a node to a list of its neighhors:
const adjList = { 'A' : [], 'B' : [] };

Graphs:

Similar to trees, if we want to dfs for something, we can just iterate and maintain a set of what we have visited, deleting from the set at the end of a node.

Graphs are pretty much the same as trees and we can reuse a lot of concepts. For instance we can mark a node as visited as soon as we enter it, and univisted at the end of the dfs call, which is usually easier than calling it for each neighbor.

Say we have some directed graph. At worst case, each node connects to every other node. That is sort of like the branching factor of the decision tree. Our path is at most n nodes. So we have n^n total paths. But I think in practice it might be more like a factorial since the decision tree drops as we visit more nodes.

If we are trying to find the shortest path to a node in an adjacency list, we can do a BFS, and worst case is V+E time.

Say that we had a linked list of letters, and we wanted to find if a letter was in the linked list. One way could be:

funtion hasLetter(node) {
  if (hasLetter(node.next)) {
    return true;
  }
  return node.val === letter;
}

Essentially, we dfs down to the end first. If anything down the chain has a letter, we bubble that result back up. Otherwise, we consider our own case.

Similarly, in dfs for graphs, we often do a similar thing. In 261, Graph Valid Tree (determine if a graph is a valid tree), we dfs out from a node, seeing if we form a tree. If at some path we are able to form a cycle / collide with the path, we bubble up that we do not form a valid tree:

for (const neighbor of neighbors) {
  if (!dfs(neighbor, node)) {
      return false;
  }
}

Essentially, we consider all neighbors, and if any neighbor had a cycle, we bubble that up. It feels a bit odd at first since the function is both having a side effect (populating a set of seen nodes, in this case) (though a side effect isn't needed), and we use the result of the function. We both call the function and use its result to determine something.

We do the same thing in 269, Alien Dictionary:

if (dfs(neighbor) === null) {
  return null;
}

Terminology:
Connected: means the graph is one piece
Undirected: means edges have no direction
acyclic: means starting from any node, it is impossible to return to that node

To detect a cycle in a directed graph (works for disconnected too):
We can iterate over each node. For each node, iterate over all paths, if we find a cycle we return that it has a cycle. If none of the nieghbors have a cycle, we return that there is no cycle. We also cache the results of safe nodes so we don't need to recompute them. The dfs occurs on each node once, and for each node, we consider all edges, so we get V+E time. Problem 207 course schedule has a good example.

When constructing a directed graph, make sure to think about which way you want the pointers to go. For instance in 207: Course Schedule, we want a node to point to the nodes that need to be taken before it.


Union find:
Without path compression or union by rank, each find and union might take O(n) time in a linked list.
With just union by rank, we have log find and union operations.
With just path compression, I think it depends on the amount of queries
With both, we get the inverse ackermann, which becomes O(1)

Advanced BFS/DP stuff:
It can perform one of the four operations at once:
1. If X is a multiple of 11, divide X by 11
2. If X is a multiple of 5, divide X by 5
3. Decrement X by 1
4. Increment X by 1
Your task is to find the minimum number of operations it will take to reduce the X to a value Y.
Constraints
1<=X<=10000
1<=Y<=10000
Y<=X
We can treat this as options on Y:
y++
y--
y * 11
y * 5

Worst case, we just increment to X. Worst case is we do X-Y ops.
So we can BFS, but any time we exceed X by the worst case, we prune, since the only way to decrease is by y--.

We could also say ops are:
x--
x++
x/5
x/11

Worst case we decrement down manually. Same thing, once we decrease too far below y, we know we can prune since theres only one way to increase X.

I think we could just use DP too and once we drop below Y we have a base case which is only increments.

BFS can take multiple params for each element in the queue, for instance: https://leetcode.com/problems/shortest-path-visiting-all-nodes/description/
We store (currNode, bitmask) in the queue, AND each element can spend N time checking all edges too. The queue has at most currNode * bitmask elements, and each takes N time to pop, so n^2 * 2^n.

0-1 Djikstra's:
Intstead of using a heap, we can just prepend things that have a weight of 0. It can be done in this question: https://leetcode.com/problems/minimum-obstacle-removal-to-reach-corner/description/

DFS VS BFS TIME:
Say we are trying to go from (0, 0) to (height - 1, width - 1) in the shortest way possible, we can move any direction, and there are walls. One way to solve this is DFS, with a path variable to prevent cycles. The issue is when we reach the end node, we don't know if we got there the fastest way. We just know we got there. We don't have a decision framework for choosing where to go, for instance:
0 0 0 0
0 0 0 0
0 0 1 0
1 1 1 0

We could go all right then all down, or we could follow down, hit a wall, go back up, etc. We don't choose which we do first, so if we do the non-shortest one first (which will probably happen), we still need to try other paths. I don't think this is a really tight bound, but a loose upper bound could be 4^(m*n), since we visit at most m*n cells and each cell has 4 options, but in practice, the options get reduced as the path gets built.

With BFS, when each cell gets visited, we know its the earliest possible, so we only consider each cell once, which is m*n time.

For instance on an infinite grid, we would never finish with DFS, but with BFS we are bounded by the true distance from the target.

Keep in mind when we can only move down or right, we can use DFS or BFS, since any path that reaches a cell is the shortest path.

This can be expanded to more complex problems, for instance: https://leetcode.com/problems/shortest-path-visiting-all-nodes/description/

Here, we have cells of (currNode, visitedBitmask), this is really no different than cells of (r, c). And just like in a grid, we can have undirected edges, so we can move back and forth. The difference is in this problem we might have up to a function of n edges for some nodes, for instance each node with mostly every node visited might be able to reach all other nodes with those same mostly every nodes visited, only difference being the current node.

So we can solve this with DFS, but like in the grid problem, it is slow, because now we consider EVERY path to go from the start to ALL (i think) nodes with a full mask. And we do this for every root node call too. My guess is because there are n*2^n nodes and each node might have up to n edges, we have n^(n*2^n) complexity, I'm not sure about this though, it blows up really fast, but maybe in practice a lot of edges don't have n nodes. I think the point is I see why it's slow, because we consider everything. Whereas in BFS, we can cache states guaranteed the first time we see them.

To do the dfs, it's no different than normal with a path:
def dfs(currNode, mask):
  # add to path here
  # recurse
  # remove from path and return

Really, this is just BACKTRACKING, as we are trying everything. The whole concept of this is backtracking, like getting every path with dfs.

Also, we can add a state "depth" which helps us terminate to otherwise forever recursive calls, but this only works if n is small, and makes the whole thing way worse as we can add a lot of total DP states. There were some solutions for shortest path visting all nodes that did this.


Say we have a DAG, and we want to answer if b is a child of a. We can do a search from a, trying to see every node. One way to think of this is at worst we consider V nodes and E edges, and there are at most V^2 edges, so it's V+V^2 = V^2 time. Or we can think of it as there are V nodes, we visit each node, each node can have V edges, so V*V = V^2. Remember we DON'T check all paths, since we just care if it is reachable. It can be used for: https://leetcode.com/problems/course-schedule-iv/
If we wanted to know it for all queries, we can use floyd warshall and make edges weight 0, otherwise infinity, I think. Or we can make edges weight 1 and also find the cheapest number of visits we need to get there! (I think, newish to floyd warshall)

Lee's code is quite strong:
class Solution:
    def checkIfPrerequisite(self, n, prerequisites, queries):
        connected = [[False] * n for i in xrange(n)]

        for i, j in prerequisites:
            connected[i][j] = True

        for k in range(n):
            for i in range(n):
                for j in range(n):
                    connected[i][j] = connected[i][j] or (connected[i][k] and connected[k][j])

        return [connected[i][j] for i, j in queries]

Instead of creating an edgemap and whatnot, he just compressed everything to connected.