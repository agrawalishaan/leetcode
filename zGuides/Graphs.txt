Matrix DFS / BFS:

Some pro tips, we can check in bounds like:

for (const [rowDiff, colDiff] of diffs) {
  const newRow = r + rowDiff;
  const newCol = c + colDiff;
  if (newRow < HEIGHT && newRow >= 0 && newCol < WIDTH && newCol >= 0 && !visited[newRow][newCol] && grid[newRow][newCol] === 1) {
      total += sizeOfIsland(newRow, newCol);
  }
}

If we don't want a huge matrix to store visited/unvisited cells, we could use a set and make keys by WIDTH*row + col. This isn't necessarily more efficient though, it depends how many keys we store. It is also slower to access. It's also harder to convert keys back and forth.

In problem 417: Pacific Atlantic Water Flow, there were a few lessons.
1) Consideer iterating from outside the cells (the edges) instead of inside. This also occured in 130: Surrounding Regions (trap islands surrounded by water)
2) Assuming we can flow from equal height cells, we need a visited set. This prevents us from caching things well though, because in:
5 5 5 5
5 3 3 1
5 5 5 5

If we solve for the second 3, we might recurse to the first 3. And the first 3 cannot flow out, because the second 3 is marked as visited. But the first 3 can actually flow out. So here, we only end up really solving for the root, and anything along it's path to the edge.
The problem itself has a good detailed explanation.

I think that between pacific atlantic water flow, and trapping rainwater II, I've seen that if we need to flow out of a cell, we can only check root cells + cells along their flow path. And finding the bottleneck height seems very hard.

In 695: Max Area of Island, we can either not recurse to invalid adjacent cells at all, or recurse to them, then return 0 if they are invalid. IMO the first is more intuitive, but many people do the second. It is similar to backtracking, just do whatever feels more natural.

In problem 695, we also don't need to set cells to be unvisited later. Since we're just trying to visit everything once, rather than find a path, we only need to set them to true.

In 827, making a large island (we can flip one water tile, find largest island), I tried writing code where we could recurse to invalid neighbors, then terminate via base cases, just for practice. Actually I think it was a bit easier.

In BFS, if we start from the top left of some island and are finding the area, or generally expanding out (for instance finding the shortest path from the top left cell to the bottom right, with walls), our queue size is limited to min(n,m), since in the worst case our queue expands as a diagonal (draw a picture of the bfs for a square matrix to see this). This can be seen in questions: 1091, shortest path in binary matrix, or 200: number of islands. I think most of these types of questions could use less space this way, since queue size is limited.
Even if we start in the middle of the grid, our BFS queue size is limited to the permiter of what we have explored, but our seen set might still be n*m space.

When we do a BFS, say to find the shortest path from one cell to another, the complexity is capped at # of cells that are within range L, where L is the distance between the two cells (this is due to the visited matrix). Whereas in DFS, we don't have certainty that we reached a cell in the shortest possible path. However, in a grid where we can only move right/down, and we want to get to the bottom right cell, DFS/BFS are the same, assuming the cost of a move is always 1. It's true that in some matrices with walls, the DFS and BFS solutions have the same cost, but in general BFS will have a lower cap than DFS (still unsure about the true DFS TC).


Adjaceny List:

Often instead we are given an adjacency list, which maps a node to a list of its neighhors:
const adjList = { 'A' : [], 'B' : [] };

Graphs:

Similar to trees, if we want to dfs for something, we can just iterate and maintain a set of what we have visited, deleting from the set at the end of a node.

Graphs are pretty much the same as trees and we can reuse a lot of concepts. For instance we can mark a node as visited as soon as we enter it, and univisted at the end of the dfs call, which is usually easier than calling it for each neighbor.

Say we have some directed graph. At worst case, each node connects to every other node. That is sort of like the branching factor of the decision tree. Our path is at most n nodes. So we have n^n total paths. But I think in practice it might be more like a factorial since the decision tree drops as we visit more nodes.

If we are trying to find the shortest path to a node in an adjacency list, we can do a BFS, and worst case is V+E time.

Say that we had a linked list of letters, and we wanted to find if a letter was in the linked list. One way could be:

funtion hasLetter(node) {
  if (hasLetter(node.next)) {
    return true;
  }
  return node.val === letter;
}

Essentially, we dfs down to the end first. If anything down the chain has a letter, we bubble that result back up. Otherwise, we consider our own case.

Similarly, in dfs for graphs, we often do a similar thing. In 261, Graph Valid Tree (determine if a graph is a valid tree), we dfs out from a node, seeing if we form a tree. If at some path we are able to form a cycle / collide with the path, we bubble up that we do not form a valid tree:

for (const neighbor of neighbors) {
  if (!dfs(neighbor, node)) {
      return false;
  }
}

Essentially, we consider all neighbors, and if any neighbor had a cycle, we bubble that up. It feels a bit odd at first since the function is both having a side effect (populating a set of seen nodes, in this case) (though a side effect isn't needed), and we use the result of the function. We both call the function and use its result to determine something.

We do the same thing in 269, Alien Dictionary:

if (dfs(neighbor) === null) {
  return null;
}

Terminology:
Connected: means the graph is one piece
Undirected: means edges have no direction
acyclic: means starting from any node, it is impossible to return to that node

To detect a cycle in a directed graph (works for disconnected too):
We can iterate over each node. For each node, iterate over all paths, if we find a cycle we return that it has a cycle. If none of the nieghbors have a cycle, we return that there is no cycle. We also cache the results of safe nodes so we don't need to recompute them. The dfs occurs on each node once, and for each node, we consider all edges, so we get V+E time. Problem 207 course schedule has a good example.

When constructing a directed graph, make sure to think about which way you want the pointers to go. For instance in 207: Course Schedule, we want a node to point to the nodes that need to be taken before it.


Union find:
Without path compression or union by rank, each find and union might take O(n) time in a linked list.
With one or the other, they are both log n.
With both, we get the inverse ackermann, which becomes O(1)
